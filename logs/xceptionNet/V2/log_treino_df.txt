Gravando log em: logs/xceptionNet/V2/log_treino_df.txt
Número de imagens no dataset de treino: 76507
Número de imagens no dataset de validação: 9563

Classes detectadas no treino: ['fake', 'real']
Mapeamento de classe para índice: {'fake': 0, 'real': 1}
/home/sato/virtualpy/lib/python3.13/site-packages/timm/models/_factory.py:138: UserWarning: Mapping deprecated model name xception to current legacy_xception.
  model = create_fn(
--- Pruning 20%
GPU: NVIDIA GeForce RTX 2060
Class weights: [0.34072697 1.6592729 ]
[Train] epoch 1/8 step 100/4782 loss_mean=0.2384
[Train] epoch 1/8 step 200/4782 loss_mean=0.2421
[Train] epoch 1/8 step 300/4782 loss_mean=0.2213
[Train] epoch 1/8 step 400/4782 loss_mean=0.2462
[Train] epoch 1/8 step 500/4782 loss_mean=0.2488
[Train] epoch 1/8 step 600/4782 loss_mean=0.2143
[Train] epoch 1/8 step 700/4782 loss_mean=0.2305
[Train] epoch 1/8 step 800/4782 loss_mean=0.2165
[Train] epoch 1/8 step 900/4782 loss_mean=0.2414
[Train] epoch 1/8 step 1000/4782 loss_mean=0.2039
[Train] epoch 1/8 step 1100/4782 loss_mean=0.2381
[Train] epoch 1/8 step 1200/4782 loss_mean=0.2260
[Train] epoch 1/8 step 1300/4782 loss_mean=0.2288
[Train] epoch 1/8 step 1400/4782 loss_mean=0.2172
[Train] epoch 1/8 step 1500/4782 loss_mean=0.2146
[Train] epoch 1/8 step 1600/4782 loss_mean=0.2127
[Train] epoch 1/8 step 1700/4782 loss_mean=0.2540
[Train] epoch 1/8 step 1800/4782 loss_mean=0.2259
[Train] epoch 1/8 step 1900/4782 loss_mean=0.2291
[Train] epoch 1/8 step 2000/4782 loss_mean=0.2045
[Train] epoch 1/8 step 2100/4782 loss_mean=0.2076
[Train] epoch 1/8 step 2200/4782 loss_mean=0.2290
[Train] epoch 1/8 step 2300/4782 loss_mean=0.1980
[Train] epoch 1/8 step 2400/4782 loss_mean=0.2221
[Train] epoch 1/8 step 2500/4782 loss_mean=0.2198
[Train] epoch 1/8 step 2600/4782 loss_mean=0.2054
[Train] epoch 1/8 step 2700/4782 loss_mean=0.2665
[Train] epoch 1/8 step 2800/4782 loss_mean=0.2286
[Train] epoch 1/8 step 2900/4782 loss_mean=0.2189
[Train] epoch 1/8 step 3000/4782 loss_mean=0.2196
[Train] epoch 1/8 step 3100/4782 loss_mean=0.2130
[Train] epoch 1/8 step 3200/4782 loss_mean=0.2243
[Train] epoch 1/8 step 3300/4782 loss_mean=0.2258
[Train] epoch 1/8 step 3400/4782 loss_mean=0.2309
[Train] epoch 1/8 step 3500/4782 loss_mean=0.2099
[Train] epoch 1/8 step 3600/4782 loss_mean=0.2506
[Train] epoch 1/8 step 3700/4782 loss_mean=0.2157
[Train] epoch 1/8 step 3800/4782 loss_mean=0.2353
[Train] epoch 1/8 step 3900/4782 loss_mean=0.2200
[Train] epoch 1/8 step 4000/4782 loss_mean=0.2141
[Train] epoch 1/8 step 4100/4782 loss_mean=0.2115
[Train] epoch 1/8 step 4200/4782 loss_mean=0.2231
[Train] epoch 1/8 step 4300/4782 loss_mean=0.2337
[Train] epoch 1/8 step 4400/4782 loss_mean=0.2349
[Train] epoch 1/8 step 4500/4782 loss_mean=0.2447
[Train] epoch 1/8 step 4600/4782 loss_mean=0.2181
[Train] epoch 1/8 step 4700/4782 loss_mean=0.2243
[Val]   epoch 1/8 step 100/598 loss_mean=0.2672
[Val]   epoch 1/8 step 200/598 loss_mean=0.2611
[Val]   epoch 1/8 step 300/598 loss_mean=0.2188
[Val]   epoch 1/8 step 400/598 loss_mean=0.2709
[Val]   epoch 1/8 step 500/598 loss_mean=0.2472
Train Loss: 0.2256 | Train Acc: 0.9093
Val Loss: 0.2785 | Val   Acc: 0.8784
 Best model saved to models/xceptionNet/V2/model_df.pt (Val Acc: 0.8784)
[Train] epoch 2/8 step 100/4782 loss_mean=0.2025
[Train] epoch 2/8 step 200/4782 loss_mean=0.2270
[Train] epoch 2/8 step 300/4782 loss_mean=0.1982
[Train] epoch 2/8 step 400/4782 loss_mean=0.1868
[Train] epoch 2/8 step 500/4782 loss_mean=0.1909
[Train] epoch 2/8 step 600/4782 loss_mean=0.2057
[Train] epoch 2/8 step 700/4782 loss_mean=0.1978
[Train] epoch 2/8 step 800/4782 loss_mean=0.2164
[Train] epoch 2/8 step 900/4782 loss_mean=0.2256
[Train] epoch 2/8 step 1000/4782 loss_mean=0.2107
[Train] epoch 2/8 step 1100/4782 loss_mean=0.2350
[Train] epoch 2/8 step 1200/4782 loss_mean=0.2374
[Train] epoch 2/8 step 1300/4782 loss_mean=0.2153
[Train] epoch 2/8 step 1400/4782 loss_mean=0.2153
[Train] epoch 2/8 step 1500/4782 loss_mean=0.1852
[Train] epoch 2/8 step 1600/4782 loss_mean=0.2255
[Train] epoch 2/8 step 1700/4782 loss_mean=0.2028
[Train] epoch 2/8 step 1800/4782 loss_mean=0.2068
[Train] epoch 2/8 step 1900/4782 loss_mean=0.2210
[Train] epoch 2/8 step 2000/4782 loss_mean=0.2252
[Train] epoch 2/8 step 2100/4782 loss_mean=0.2057
[Train] epoch 2/8 step 2200/4782 loss_mean=0.1997
[Train] epoch 2/8 step 2300/4782 loss_mean=0.2188
[Train] epoch 2/8 step 2400/4782 loss_mean=0.1831
[Train] epoch 2/8 step 2500/4782 loss_mean=0.2272
[Train] epoch 2/8 step 2600/4782 loss_mean=0.2031
[Train] epoch 2/8 step 2700/4782 loss_mean=0.1926
[Train] epoch 2/8 step 2800/4782 loss_mean=0.1754
[Train] epoch 2/8 step 2900/4782 loss_mean=0.1980
[Train] epoch 2/8 step 3000/4782 loss_mean=0.2132
[Train] epoch 2/8 step 3100/4782 loss_mean=0.2009
[Train] epoch 2/8 step 3200/4782 loss_mean=0.1828
[Train] epoch 2/8 step 3300/4782 loss_mean=0.1913
[Train] epoch 2/8 step 3400/4782 loss_mean=0.2384
[Train] epoch 2/8 step 3500/4782 loss_mean=0.2080
[Train] epoch 2/8 step 3600/4782 loss_mean=0.2088
[Train] epoch 2/8 step 3700/4782 loss_mean=0.2149
[Train] epoch 2/8 step 3800/4782 loss_mean=0.1849
[Train] epoch 2/8 step 3900/4782 loss_mean=0.2083
[Train] epoch 2/8 step 4000/4782 loss_mean=0.2006
[Train] epoch 2/8 step 4100/4782 loss_mean=0.1865
[Train] epoch 2/8 step 4200/4782 loss_mean=0.1993
[Train] epoch 2/8 step 4300/4782 loss_mean=0.2200
[Train] epoch 2/8 step 4400/4782 loss_mean=0.1692
[Train] epoch 2/8 step 4500/4782 loss_mean=0.2217
[Train] epoch 2/8 step 4600/4782 loss_mean=0.2176
[Train] epoch 2/8 step 4700/4782 loss_mean=0.2041
[Val]   epoch 2/8 step 100/598 loss_mean=0.2829
[Val]   epoch 2/8 step 200/598 loss_mean=0.2612
[Val]   epoch 2/8 step 300/598 loss_mean=0.2328
[Val]   epoch 2/8 step 400/598 loss_mean=0.2856
[Val]   epoch 2/8 step 500/598 loss_mean=0.2600
Train Loss: 0.2068 | Train Acc: 0.9168
Val Loss: 0.2798 | Val   Acc: 0.8775
[Train] epoch 3/8 step 100/4782 loss_mean=0.1885
[Train] epoch 3/8 step 200/4782 loss_mean=0.2115
[Train] epoch 3/8 step 300/4782 loss_mean=0.1916
[Train] epoch 3/8 step 400/4782 loss_mean=0.1775
[Train] epoch 3/8 step 500/4782 loss_mean=0.1898
[Train] epoch 3/8 step 600/4782 loss_mean=0.2163
[Train] epoch 3/8 step 700/4782 loss_mean=0.2188
[Train] epoch 3/8 step 800/4782 loss_mean=0.1962
[Train] epoch 3/8 step 900/4782 loss_mean=0.1835
[Train] epoch 3/8 step 1000/4782 loss_mean=0.1868
[Train] epoch 3/8 step 1100/4782 loss_mean=0.1784
[Train] epoch 3/8 step 1200/4782 loss_mean=0.1972
[Train] epoch 3/8 step 1300/4782 loss_mean=0.1929
[Train] epoch 3/8 step 1400/4782 loss_mean=0.1804
[Train] epoch 3/8 step 1500/4782 loss_mean=0.1816
[Train] epoch 3/8 step 1600/4782 loss_mean=0.1900
[Train] epoch 3/8 step 1700/4782 loss_mean=0.1577
[Train] epoch 3/8 step 1800/4782 loss_mean=0.1883
[Train] epoch 3/8 step 1900/4782 loss_mean=0.1813
[Train] epoch 3/8 step 2000/4782 loss_mean=0.1940
[Train] epoch 3/8 step 2100/4782 loss_mean=0.1957
[Train] epoch 3/8 step 2200/4782 loss_mean=0.1978
[Train] epoch 3/8 step 2300/4782 loss_mean=0.2007
[Train] epoch 3/8 step 2400/4782 loss_mean=0.1759
[Train] epoch 3/8 step 2500/4782 loss_mean=0.1801
[Train] epoch 3/8 step 2600/4782 loss_mean=0.1828
[Train] epoch 3/8 step 2700/4782 loss_mean=0.1978
[Train] epoch 3/8 step 2800/4782 loss_mean=0.1662
[Train] epoch 3/8 step 2900/4782 loss_mean=0.1839
[Train] epoch 3/8 step 3000/4782 loss_mean=0.2054
[Train] epoch 3/8 step 3100/4782 loss_mean=0.1897
[Train] epoch 3/8 step 3200/4782 loss_mean=0.1897
[Train] epoch 3/8 step 3300/4782 loss_mean=0.2158
[Train] epoch 3/8 step 3400/4782 loss_mean=0.1938
[Train] epoch 3/8 step 3500/4782 loss_mean=0.2055
[Train] epoch 3/8 step 3600/4782 loss_mean=0.1960
[Train] epoch 3/8 step 3700/4782 loss_mean=0.1993
[Train] epoch 3/8 step 3800/4782 loss_mean=0.2026
[Train] epoch 3/8 step 3900/4782 loss_mean=0.1812
[Train] epoch 3/8 step 4000/4782 loss_mean=0.1970
[Train] epoch 3/8 step 4100/4782 loss_mean=0.2212
[Train] epoch 3/8 step 4200/4782 loss_mean=0.1708
[Train] epoch 3/8 step 4300/4782 loss_mean=0.1779
[Train] epoch 3/8 step 4400/4782 loss_mean=0.1915
[Train] epoch 3/8 step 4500/4782 loss_mean=0.2111
[Train] epoch 3/8 step 4600/4782 loss_mean=0.1825
[Train] epoch 3/8 step 4700/4782 loss_mean=0.2046
[Val]   epoch 3/8 step 100/598 loss_mean=0.2611
[Val]   epoch 3/8 step 200/598 loss_mean=0.2539
[Val]   epoch 3/8 step 300/598 loss_mean=0.2155
[Val]   epoch 3/8 step 400/598 loss_mean=0.2634
[Val]   epoch 3/8 step 500/598 loss_mean=0.2337
Train Loss: 0.1922 | Train Acc: 0.9217
Val Loss: 0.2661 | Val   Acc: 0.8877
 Best model saved to models/xceptionNet/V2/model_df.pt (Val Acc: 0.8877)
[Train] epoch 4/8 step 100/4782 loss_mean=0.1748
[Train] epoch 4/8 step 200/4782 loss_mean=0.2020
[Train] epoch 4/8 step 300/4782 loss_mean=0.1645
[Train] epoch 4/8 step 400/4782 loss_mean=0.1753
[Train] epoch 4/8 step 500/4782 loss_mean=0.1773
[Train] epoch 4/8 step 600/4782 loss_mean=0.1667
[Train] epoch 4/8 step 700/4782 loss_mean=0.1928
[Train] epoch 4/8 step 800/4782 loss_mean=0.1784
[Train] epoch 4/8 step 900/4782 loss_mean=0.1984
[Train] epoch 4/8 step 1000/4782 loss_mean=0.2086
[Train] epoch 4/8 step 1100/4782 loss_mean=0.1793
[Train] epoch 4/8 step 1200/4782 loss_mean=0.1865
[Train] epoch 4/8 step 1300/4782 loss_mean=0.1731
[Train] epoch 4/8 step 1400/4782 loss_mean=0.1618
[Train] epoch 4/8 step 1500/4782 loss_mean=0.1890
[Train] epoch 4/8 step 1600/4782 loss_mean=0.1697
[Train] epoch 4/8 step 1700/4782 loss_mean=0.1771
[Train] epoch 4/8 step 1800/4782 loss_mean=0.1871
[Train] epoch 4/8 step 1900/4782 loss_mean=0.1708
[Train] epoch 4/8 step 2000/4782 loss_mean=0.1969
[Train] epoch 4/8 step 2100/4782 loss_mean=0.1765
[Train] epoch 4/8 step 2200/4782 loss_mean=0.2089
[Train] epoch 4/8 step 2300/4782 loss_mean=0.1819
[Train] epoch 4/8 step 2400/4782 loss_mean=0.2000
[Train] epoch 4/8 step 2500/4782 loss_mean=0.1614
[Train] epoch 4/8 step 2600/4782 loss_mean=0.1501
[Train] epoch 4/8 step 2700/4782 loss_mean=0.1769
[Train] epoch 4/8 step 2800/4782 loss_mean=0.1973
[Train] epoch 4/8 step 2900/4782 loss_mean=0.2057
[Train] epoch 4/8 step 3000/4782 loss_mean=0.1787
[Train] epoch 4/8 step 3100/4782 loss_mean=0.1909
[Train] epoch 4/8 step 3200/4782 loss_mean=0.1926
[Train] epoch 4/8 step 3300/4782 loss_mean=0.1844
[Train] epoch 4/8 step 3400/4782 loss_mean=0.1752
[Train] epoch 4/8 step 3500/4782 loss_mean=0.1854
[Train] epoch 4/8 step 3600/4782 loss_mean=0.1849
[Train] epoch 4/8 step 3700/4782 loss_mean=0.1785
[Train] epoch 4/8 step 3800/4782 loss_mean=0.1879
[Train] epoch 4/8 step 3900/4782 loss_mean=0.1822
[Train] epoch 4/8 step 4000/4782 loss_mean=0.1858
[Train] epoch 4/8 step 4100/4782 loss_mean=0.1817
[Train] epoch 4/8 step 4200/4782 loss_mean=0.1595
[Train] epoch 4/8 step 4300/4782 loss_mean=0.2098
[Train] epoch 4/8 step 4400/4782 loss_mean=0.1721
[Train] epoch 4/8 step 4500/4782 loss_mean=0.1728
[Train] epoch 4/8 step 4600/4782 loss_mean=0.1728
[Train] epoch 4/8 step 4700/4782 loss_mean=0.1809
[Val]   epoch 4/8 step 100/598 loss_mean=0.2128
[Val]   epoch 4/8 step 200/598 loss_mean=0.2028
[Val]   epoch 4/8 step 300/598 loss_mean=0.1715
[Val]   epoch 4/8 step 400/598 loss_mean=0.2203
[Val]   epoch 4/8 step 500/598 loss_mean=0.2068
Train Loss: 0.1822 | Train Acc: 0.9270
Val Loss: 0.2437 | Val   Acc: 0.8984
 Best model saved to models/xceptionNet/V2/model_df.pt (Val Acc: 0.8984)
[Train] epoch 5/8 step 100/4782 loss_mean=0.1720
[Train] epoch 5/8 step 200/4782 loss_mean=0.1686
[Train] epoch 5/8 step 300/4782 loss_mean=0.1715
[Train] epoch 5/8 step 400/4782 loss_mean=0.2029
[Train] epoch 5/8 step 500/4782 loss_mean=0.1559
[Train] epoch 5/8 step 600/4782 loss_mean=0.1830
[Train] epoch 5/8 step 700/4782 loss_mean=0.1477
[Train] epoch 5/8 step 800/4782 loss_mean=0.1341
[Train] epoch 5/8 step 900/4782 loss_mean=0.1716
[Train] epoch 5/8 step 1000/4782 loss_mean=0.1352
[Train] epoch 5/8 step 1100/4782 loss_mean=0.1858
[Train] epoch 5/8 step 1200/4782 loss_mean=0.1731
[Train] epoch 5/8 step 1300/4782 loss_mean=0.1895
[Train] epoch 5/8 step 1400/4782 loss_mean=0.1546
[Train] epoch 5/8 step 1500/4782 loss_mean=0.1758
[Train] epoch 5/8 step 1600/4782 loss_mean=0.1766
[Train] epoch 5/8 step 1700/4782 loss_mean=0.1784
[Train] epoch 5/8 step 1800/4782 loss_mean=0.1589
[Train] epoch 5/8 step 1900/4782 loss_mean=0.1788
[Train] epoch 5/8 step 2000/4782 loss_mean=0.1562
[Train] epoch 5/8 step 2100/4782 loss_mean=0.1822
[Train] epoch 5/8 step 2200/4782 loss_mean=0.1878
[Train] epoch 5/8 step 2300/4782 loss_mean=0.1702
[Train] epoch 5/8 step 2400/4782 loss_mean=0.1815
[Train] epoch 5/8 step 2500/4782 loss_mean=0.1689
[Train] epoch 5/8 step 2600/4782 loss_mean=0.1663
[Train] epoch 5/8 step 2700/4782 loss_mean=0.1707
[Train] epoch 5/8 step 2800/4782 loss_mean=0.1610
[Train] epoch 5/8 step 2900/4782 loss_mean=0.1746
[Train] epoch 5/8 step 3000/4782 loss_mean=0.1948
[Train] epoch 5/8 step 3100/4782 loss_mean=0.1790
[Train] epoch 5/8 step 3200/4782 loss_mean=0.1914
[Train] epoch 5/8 step 3300/4782 loss_mean=0.1855
[Train] epoch 5/8 step 3400/4782 loss_mean=0.1764
[Train] epoch 5/8 step 3500/4782 loss_mean=0.1569
[Train] epoch 5/8 step 3600/4782 loss_mean=0.1921
[Train] epoch 5/8 step 3700/4782 loss_mean=0.1783
[Train] epoch 5/8 step 3800/4782 loss_mean=0.1906
[Train] epoch 5/8 step 3900/4782 loss_mean=0.1599
[Train] epoch 5/8 step 4000/4782 loss_mean=0.1834
[Train] epoch 5/8 step 4100/4782 loss_mean=0.1644
[Train] epoch 5/8 step 4200/4782 loss_mean=0.1577
[Train] epoch 5/8 step 4300/4782 loss_mean=0.1659
[Train] epoch 5/8 step 4400/4782 loss_mean=0.1872
[Train] epoch 5/8 step 4500/4782 loss_mean=0.1494
[Train] epoch 5/8 step 4600/4782 loss_mean=0.1918
[Train] epoch 5/8 step 4700/4782 loss_mean=0.1453
[Val]   epoch 5/8 step 100/598 loss_mean=0.2092
[Val]   epoch 5/8 step 200/598 loss_mean=0.2051
[Val]   epoch 5/8 step 300/598 loss_mean=0.1674
[Val]   epoch 5/8 step 400/598 loss_mean=0.2192
[Val]   epoch 5/8 step 500/598 loss_mean=0.1962
Train Loss: 0.1718 | Train Acc: 0.9318
Val Loss: 0.2381 | Val   Acc: 0.8995
 Best model saved to models/xceptionNet/V2/model_df.pt (Val Acc: 0.8995)
[Train] epoch 6/8 step 100/4782 loss_mean=0.1628
[Train] epoch 6/8 step 200/4782 loss_mean=0.1604
[Train] epoch 6/8 step 300/4782 loss_mean=0.1612
[Train] epoch 6/8 step 400/4782 loss_mean=0.1403
[Train] epoch 6/8 step 500/4782 loss_mean=0.1517
[Train] epoch 6/8 step 600/4782 loss_mean=0.1531
[Train] epoch 6/8 step 700/4782 loss_mean=0.1712
[Train] epoch 6/8 step 800/4782 loss_mean=0.1487
[Train] epoch 6/8 step 900/4782 loss_mean=0.1395
[Train] epoch 6/8 step 1000/4782 loss_mean=0.1486
[Train] epoch 6/8 step 1100/4782 loss_mean=0.1707
[Train] epoch 6/8 step 1200/4782 loss_mean=0.1535
[Train] epoch 6/8 step 1300/4782 loss_mean=0.1545
[Train] epoch 6/8 step 1400/4782 loss_mean=0.1427
[Train] epoch 6/8 step 1500/4782 loss_mean=0.1720
[Train] epoch 6/8 step 1600/4782 loss_mean=0.1598
[Train] epoch 6/8 step 1700/4782 loss_mean=0.1746
[Train] epoch 6/8 step 1800/4782 loss_mean=0.1342
[Train] epoch 6/8 step 1900/4782 loss_mean=0.1501
[Train] epoch 6/8 step 2000/4782 loss_mean=0.1597
[Train] epoch 6/8 step 2100/4782 loss_mean=0.1641
[Train] epoch 6/8 step 2200/4782 loss_mean=0.1437
[Train] epoch 6/8 step 2300/4782 loss_mean=0.1560
[Train] epoch 6/8 step 2400/4782 loss_mean=0.1330
[Train] epoch 6/8 step 2500/4782 loss_mean=0.1255
[Train] epoch 6/8 step 2600/4782 loss_mean=0.1577
[Train] epoch 6/8 step 2700/4782 loss_mean=0.1441
[Train] epoch 6/8 step 2800/4782 loss_mean=0.1538
[Train] epoch 6/8 step 2900/4782 loss_mean=0.1487
[Train] epoch 6/8 step 3000/4782 loss_mean=0.1610
[Train] epoch 6/8 step 3100/4782 loss_mean=0.1402
[Train] epoch 6/8 step 3200/4782 loss_mean=0.1302
[Train] epoch 6/8 step 3300/4782 loss_mean=0.1510
[Train] epoch 6/8 step 3400/4782 loss_mean=0.1523
[Train] epoch 6/8 step 3500/4782 loss_mean=0.1271
[Train] epoch 6/8 step 3600/4782 loss_mean=0.1657
[Train] epoch 6/8 step 3700/4782 loss_mean=0.1749
[Train] epoch 6/8 step 3800/4782 loss_mean=0.1311
[Train] epoch 6/8 step 3900/4782 loss_mean=0.1376
[Train] epoch 6/8 step 4000/4782 loss_mean=0.1648
[Train] epoch 6/8 step 4100/4782 loss_mean=0.1672
[Train] epoch 6/8 step 4200/4782 loss_mean=0.1732
[Train] epoch 6/8 step 4300/4782 loss_mean=0.1428
[Train] epoch 6/8 step 4400/4782 loss_mean=0.1382
[Train] epoch 6/8 step 4500/4782 loss_mean=0.1340
[Train] epoch 6/8 step 4600/4782 loss_mean=0.1331
[Train] epoch 6/8 step 4700/4782 loss_mean=0.1299
[Val]   epoch 6/8 step 100/598 loss_mean=0.2465
[Val]   epoch 6/8 step 200/598 loss_mean=0.2337
[Val]   epoch 6/8 step 300/598 loss_mean=0.1875
[Val]   epoch 6/8 step 400/598 loss_mean=0.2437
[Val]   epoch 6/8 step 500/598 loss_mean=0.2246
Train Loss: 0.1510 | Train Acc: 0.9412
Val Loss: 0.2566 | Val   Acc: 0.8950
[Train] epoch 7/8 step 100/4782 loss_mean=0.1524
[Train] epoch 7/8 step 200/4782 loss_mean=0.1303
[Train] epoch 7/8 step 300/4782 loss_mean=0.1644
[Train] epoch 7/8 step 400/4782 loss_mean=0.1548
[Train] epoch 7/8 step 500/4782 loss_mean=0.1219
[Train] epoch 7/8 step 600/4782 loss_mean=0.1380
[Train] epoch 7/8 step 700/4782 loss_mean=0.1368
[Train] epoch 7/8 step 800/4782 loss_mean=0.1288
[Train] epoch 7/8 step 900/4782 loss_mean=0.1560
[Train] epoch 7/8 step 1000/4782 loss_mean=0.1630
[Train] epoch 7/8 step 1100/4782 loss_mean=0.1380
[Train] epoch 7/8 step 1200/4782 loss_mean=0.1495
[Train] epoch 7/8 step 1300/4782 loss_mean=0.1451
[Train] epoch 7/8 step 1400/4782 loss_mean=0.1456
[Train] epoch 7/8 step 1500/4782 loss_mean=0.1291
[Train] epoch 7/8 step 1600/4782 loss_mean=0.1541
[Train] epoch 7/8 step 1700/4782 loss_mean=0.1426
[Train] epoch 7/8 step 1800/4782 loss_mean=0.1347
[Train] epoch 7/8 step 1900/4782 loss_mean=0.1570
[Train] epoch 7/8 step 2000/4782 loss_mean=0.1682
[Train] epoch 7/8 step 2100/4782 loss_mean=0.1397
[Train] epoch 7/8 step 2200/4782 loss_mean=0.1511
[Train] epoch 7/8 step 2300/4782 loss_mean=0.1273
[Train] epoch 7/8 step 2400/4782 loss_mean=0.1644
[Train] epoch 7/8 step 2500/4782 loss_mean=0.1264
[Train] epoch 7/8 step 2600/4782 loss_mean=0.1492
[Train] epoch 7/8 step 2700/4782 loss_mean=0.1426
[Train] epoch 7/8 step 2800/4782 loss_mean=0.1478
[Train] epoch 7/8 step 2900/4782 loss_mean=0.1560
[Train] epoch 7/8 step 3000/4782 loss_mean=0.1201
[Train] epoch 7/8 step 3100/4782 loss_mean=0.1618
[Train] epoch 7/8 step 3200/4782 loss_mean=0.1557
[Train] epoch 7/8 step 3300/4782 loss_mean=0.1522
[Train] epoch 7/8 step 3400/4782 loss_mean=0.1445
[Train] epoch 7/8 step 3500/4782 loss_mean=0.1312
[Train] epoch 7/8 step 3600/4782 loss_mean=0.1232
[Train] epoch 7/8 step 3700/4782 loss_mean=0.1384
[Train] epoch 7/8 step 3800/4782 loss_mean=0.1633
[Train] epoch 7/8 step 3900/4782 loss_mean=0.1546
[Train] epoch 7/8 step 4000/4782 loss_mean=0.1328
[Train] epoch 7/8 step 4100/4782 loss_mean=0.1361
[Train] epoch 7/8 step 4200/4782 loss_mean=0.1351
[Train] epoch 7/8 step 4300/4782 loss_mean=0.1443
[Train] epoch 7/8 step 4400/4782 loss_mean=0.1504
[Train] epoch 7/8 step 4500/4782 loss_mean=0.1301
[Train] epoch 7/8 step 4600/4782 loss_mean=0.1325
[Train] epoch 7/8 step 4700/4782 loss_mean=0.1396
[Val]   epoch 7/8 step 100/598 loss_mean=0.1913
[Val]   epoch 7/8 step 200/598 loss_mean=0.1931
[Val]   epoch 7/8 step 300/598 loss_mean=0.1478
[Val]   epoch 7/8 step 400/598 loss_mean=0.1940
[Val]   epoch 7/8 step 500/598 loss_mean=0.1842
Train Loss: 0.1437 | Train Acc: 0.9450
Val Loss: 0.2289 | Val   Acc: 0.9059
 Best model saved to models/xceptionNet/V2/model_df.pt (Val Acc: 0.9059)
[Train] epoch 8/8 step 100/4782 loss_mean=0.1294
[Train] epoch 8/8 step 200/4782 loss_mean=0.1113
[Train] epoch 8/8 step 300/4782 loss_mean=0.1280
[Train] epoch 8/8 step 400/4782 loss_mean=0.1215
[Train] epoch 8/8 step 500/4782 loss_mean=0.1546
[Train] epoch 8/8 step 600/4782 loss_mean=0.1553
[Train] epoch 8/8 step 700/4782 loss_mean=0.1345
[Train] epoch 8/8 step 800/4782 loss_mean=0.1239
[Train] epoch 8/8 step 900/4782 loss_mean=0.1216
[Train] epoch 8/8 step 1000/4782 loss_mean=0.1340
[Train] epoch 8/8 step 1100/4782 loss_mean=0.1041
[Train] epoch 8/8 step 1200/4782 loss_mean=0.1199
[Train] epoch 8/8 step 1300/4782 loss_mean=0.1353
[Train] epoch 8/8 step 1400/4782 loss_mean=0.1327
[Train] epoch 8/8 step 1500/4782 loss_mean=0.1234
[Train] epoch 8/8 step 1600/4782 loss_mean=0.1434
[Train] epoch 8/8 step 1700/4782 loss_mean=0.1269
[Train] epoch 8/8 step 1800/4782 loss_mean=0.1292
[Train] epoch 8/8 step 1900/4782 loss_mean=0.1170
[Train] epoch 8/8 step 2000/4782 loss_mean=0.1358
[Train] epoch 8/8 step 2100/4782 loss_mean=0.1081
[Train] epoch 8/8 step 2200/4782 loss_mean=0.1371
[Train] epoch 8/8 step 2300/4782 loss_mean=0.1482
[Train] epoch 8/8 step 2400/4782 loss_mean=0.1358
[Train] epoch 8/8 step 2500/4782 loss_mean=0.1148
[Train] epoch 8/8 step 2600/4782 loss_mean=0.1256
[Train] epoch 8/8 step 2700/4782 loss_mean=0.1370
[Train] epoch 8/8 step 2800/4782 loss_mean=0.1445
[Train] epoch 8/8 step 2900/4782 loss_mean=0.1146
[Train] epoch 8/8 step 3000/4782 loss_mean=0.1462
[Train] epoch 8/8 step 3100/4782 loss_mean=0.1295
[Train] epoch 8/8 step 3200/4782 loss_mean=0.1296
[Train] epoch 8/8 step 3300/4782 loss_mean=0.1282
[Train] epoch 8/8 step 3400/4782 loss_mean=0.1445
[Train] epoch 8/8 step 3500/4782 loss_mean=0.1147
[Train] epoch 8/8 step 3600/4782 loss_mean=0.1365
[Train] epoch 8/8 step 3700/4782 loss_mean=0.1320
[Train] epoch 8/8 step 3800/4782 loss_mean=0.1361
[Train] epoch 8/8 step 3900/4782 loss_mean=0.1392
[Train] epoch 8/8 step 4000/4782 loss_mean=0.1294
[Train] epoch 8/8 step 4100/4782 loss_mean=0.1392
[Train] epoch 8/8 step 4200/4782 loss_mean=0.1552
[Train] epoch 8/8 step 4300/4782 loss_mean=0.1476
