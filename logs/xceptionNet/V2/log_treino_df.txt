Gravando log em: logs/xceptionNet/V2/log_treino_df.txt
Número de imagens no dataset de treino: 76507
Número de imagens no dataset de validação: 9563

Classes detectadas no treino: ['fake', 'real']
Mapeamento de classe para índice: {'fake': 0, 'real': 1}
/home/sato/virtualpy/lib/python3.13/site-packages/timm/models/_factory.py:138: UserWarning: Mapping deprecated model name xception to current legacy_xception.
  model = create_fn(
--- Pruning 20%
GPU: NVIDIA GeForce RTX 2060
Class weights: [0.34072697 1.6592729 ]
[Train] epoch 1/8 step 100/4782 loss_mean=0.2384
[Train] epoch 1/8 step 200/4782 loss_mean=0.2421
[Train] epoch 1/8 step 300/4782 loss_mean=0.2213
[Train] epoch 1/8 step 400/4782 loss_mean=0.2462
[Train] epoch 1/8 step 500/4782 loss_mean=0.2488
[Train] epoch 1/8 step 600/4782 loss_mean=0.2143
[Train] epoch 1/8 step 700/4782 loss_mean=0.2305
[Train] epoch 1/8 step 800/4782 loss_mean=0.2165
[Train] epoch 1/8 step 900/4782 loss_mean=0.2414
[Train] epoch 1/8 step 1000/4782 loss_mean=0.2039
[Train] epoch 1/8 step 1100/4782 loss_mean=0.2381
[Train] epoch 1/8 step 1200/4782 loss_mean=0.2260
[Train] epoch 1/8 step 1300/4782 loss_mean=0.2288
[Train] epoch 1/8 step 1400/4782 loss_mean=0.2172
[Train] epoch 1/8 step 1500/4782 loss_mean=0.2146
[Train] epoch 1/8 step 1600/4782 loss_mean=0.2127
[Train] epoch 1/8 step 1700/4782 loss_mean=0.2540
[Train] epoch 1/8 step 1800/4782 loss_mean=0.2259
[Train] epoch 1/8 step 1900/4782 loss_mean=0.2291
[Train] epoch 1/8 step 2000/4782 loss_mean=0.2045
[Train] epoch 1/8 step 2100/4782 loss_mean=0.2076
[Train] epoch 1/8 step 2200/4782 loss_mean=0.2290
[Train] epoch 1/8 step 2300/4782 loss_mean=0.1980
[Train] epoch 1/8 step 2400/4782 loss_mean=0.2221
[Train] epoch 1/8 step 2500/4782 loss_mean=0.2198
[Train] epoch 1/8 step 2600/4782 loss_mean=0.2054
[Train] epoch 1/8 step 2700/4782 loss_mean=0.2665
[Train] epoch 1/8 step 2800/4782 loss_mean=0.2286
[Train] epoch 1/8 step 2900/4782 loss_mean=0.2189
[Train] epoch 1/8 step 3000/4782 loss_mean=0.2196
[Train] epoch 1/8 step 3100/4782 loss_mean=0.2130
[Train] epoch 1/8 step 3200/4782 loss_mean=0.2243
[Train] epoch 1/8 step 3300/4782 loss_mean=0.2258
[Train] epoch 1/8 step 3400/4782 loss_mean=0.2309
[Train] epoch 1/8 step 3500/4782 loss_mean=0.2099
[Train] epoch 1/8 step 3600/4782 loss_mean=0.2506
[Train] epoch 1/8 step 3700/4782 loss_mean=0.2157
[Train] epoch 1/8 step 3800/4782 loss_mean=0.2353
[Train] epoch 1/8 step 3900/4782 loss_mean=0.2200
[Train] epoch 1/8 step 4000/4782 loss_mean=0.2141
[Train] epoch 1/8 step 4100/4782 loss_mean=0.2115
[Train] epoch 1/8 step 4200/4782 loss_mean=0.2231
[Train] epoch 1/8 step 4300/4782 loss_mean=0.2337
[Train] epoch 1/8 step 4400/4782 loss_mean=0.2349
[Train] epoch 1/8 step 4500/4782 loss_mean=0.2447
[Train] epoch 1/8 step 4600/4782 loss_mean=0.2181
[Train] epoch 1/8 step 4700/4782 loss_mean=0.2243
[Val]   epoch 1/8 step 100/598 loss_mean=0.2672
[Val]   epoch 1/8 step 200/598 loss_mean=0.2611
[Val]   epoch 1/8 step 300/598 loss_mean=0.2188
[Val]   epoch 1/8 step 400/598 loss_mean=0.2709
[Val]   epoch 1/8 step 500/598 loss_mean=0.2472
Train Loss: 0.2256 | Train Acc: 0.9093
Val Loss: 0.2785 | Val   Acc: 0.8784
 Best model saved to models/xceptionNet/V2/model_df.pt (Val Acc: 0.8784)
[Train] epoch 2/8 step 100/4782 loss_mean=0.2025
[Train] epoch 2/8 step 200/4782 loss_mean=0.2270
[Train] epoch 2/8 step 300/4782 loss_mean=0.1982
[Train] epoch 2/8 step 400/4782 loss_mean=0.1868
[Train] epoch 2/8 step 500/4782 loss_mean=0.1909
[Train] epoch 2/8 step 600/4782 loss_mean=0.2057
[Train] epoch 2/8 step 700/4782 loss_mean=0.1978
[Train] epoch 2/8 step 800/4782 loss_mean=0.2164
[Train] epoch 2/8 step 900/4782 loss_mean=0.2256
[Train] epoch 2/8 step 1000/4782 loss_mean=0.2107
[Train] epoch 2/8 step 1100/4782 loss_mean=0.2350
[Train] epoch 2/8 step 1200/4782 loss_mean=0.2374
[Train] epoch 2/8 step 1300/4782 loss_mean=0.2153
[Train] epoch 2/8 step 1400/4782 loss_mean=0.2153
[Train] epoch 2/8 step 1500/4782 loss_mean=0.1852
[Train] epoch 2/8 step 1600/4782 loss_mean=0.2255
[Train] epoch 2/8 step 1700/4782 loss_mean=0.2028
[Train] epoch 2/8 step 1800/4782 loss_mean=0.2068
[Train] epoch 2/8 step 1900/4782 loss_mean=0.2210
[Train] epoch 2/8 step 2000/4782 loss_mean=0.2252
[Train] epoch 2/8 step 2100/4782 loss_mean=0.2057
[Train] epoch 2/8 step 2200/4782 loss_mean=0.1997
[Train] epoch 2/8 step 2300/4782 loss_mean=0.2188
[Train] epoch 2/8 step 2400/4782 loss_mean=0.1831
[Train] epoch 2/8 step 2500/4782 loss_mean=0.2272
[Train] epoch 2/8 step 2600/4782 loss_mean=0.2031
[Train] epoch 2/8 step 2700/4782 loss_mean=0.1926
[Train] epoch 2/8 step 2800/4782 loss_mean=0.1754
[Train] epoch 2/8 step 2900/4782 loss_mean=0.1980
[Train] epoch 2/8 step 3000/4782 loss_mean=0.2132
[Train] epoch 2/8 step 3100/4782 loss_mean=0.2009
[Train] epoch 2/8 step 3200/4782 loss_mean=0.1828
[Train] epoch 2/8 step 3300/4782 loss_mean=0.1913
[Train] epoch 2/8 step 3400/4782 loss_mean=0.2384
[Train] epoch 2/8 step 3500/4782 loss_mean=0.2080
[Train] epoch 2/8 step 3600/4782 loss_mean=0.2088
[Train] epoch 2/8 step 3700/4782 loss_mean=0.2149
[Train] epoch 2/8 step 3800/4782 loss_mean=0.1849
[Train] epoch 2/8 step 3900/4782 loss_mean=0.2083
[Train] epoch 2/8 step 4000/4782 loss_mean=0.2006
[Train] epoch 2/8 step 4100/4782 loss_mean=0.1865
[Train] epoch 2/8 step 4200/4782 loss_mean=0.1993
[Train] epoch 2/8 step 4300/4782 loss_mean=0.2200
[Train] epoch 2/8 step 4400/4782 loss_mean=0.1692
[Train] epoch 2/8 step 4500/4782 loss_mean=0.2217
[Train] epoch 2/8 step 4600/4782 loss_mean=0.2176
[Train] epoch 2/8 step 4700/4782 loss_mean=0.2041
[Val]   epoch 2/8 step 100/598 loss_mean=0.2829
[Val]   epoch 2/8 step 200/598 loss_mean=0.2612
[Val]   epoch 2/8 step 300/598 loss_mean=0.2328
[Val]   epoch 2/8 step 400/598 loss_mean=0.2856
[Val]   epoch 2/8 step 500/598 loss_mean=0.2600
Train Loss: 0.2068 | Train Acc: 0.9168
Val Loss: 0.2798 | Val   Acc: 0.8775
[Train] epoch 3/8 step 100/4782 loss_mean=0.1885
[Train] epoch 3/8 step 200/4782 loss_mean=0.2115
[Train] epoch 3/8 step 300/4782 loss_mean=0.1916
[Train] epoch 3/8 step 400/4782 loss_mean=0.1775
[Train] epoch 3/8 step 500/4782 loss_mean=0.1898
[Train] epoch 3/8 step 600/4782 loss_mean=0.2163
[Train] epoch 3/8 step 700/4782 loss_mean=0.2188
[Train] epoch 3/8 step 800/4782 loss_mean=0.1962
[Train] epoch 3/8 step 900/4782 loss_mean=0.1835
[Train] epoch 3/8 step 1000/4782 loss_mean=0.1868
[Train] epoch 3/8 step 1100/4782 loss_mean=0.1784
[Train] epoch 3/8 step 1200/4782 loss_mean=0.1972
[Train] epoch 3/8 step 1300/4782 loss_mean=0.1929
