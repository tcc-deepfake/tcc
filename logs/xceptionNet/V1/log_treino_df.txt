Gravando log em: logs/xceptionNet/V1/log_treino_df.txt
Número de imagens no dataset de treino: 76507
Número de imagens no dataset de validação: 9563

Classes detectadas no treino: ['fake', 'real']
Mapeamento de classe para índice: {'fake': 0, 'real': 1}
/home/sato/virtualpy/lib/python3.13/site-packages/timm/models/_factory.py:138: UserWarning: Mapping deprecated model name xception to current legacy_xception.
  model = create_fn(
GPU: NVIDIA GeForce RTX 2060
Parametros treináveis:
  fc.weight
  fc.bias

EPOCH 1/10
------------------------------
[Train] epoch 1/10 step 100/2391 loss_mean=0.4920
[Train] epoch 1/10 step 200/2391 loss_mean=0.4589
[Train] epoch 1/10 step 300/2391 loss_mean=0.4604
[Train] epoch 1/10 step 400/2391 loss_mean=0.4551
[Train] epoch 1/10 step 500/2391 loss_mean=0.4415
[Train] epoch 1/10 step 600/2391 loss_mean=0.4558
[Train] epoch 1/10 step 700/2391 loss_mean=0.4534
[Train] epoch 1/10 step 800/2391 loss_mean=0.4489
[Train] epoch 1/10 step 900/2391 loss_mean=0.4417
[Train] epoch 1/10 step 1000/2391 loss_mean=0.4562
[Train] epoch 1/10 step 1100/2391 loss_mean=0.4605
[Train] epoch 1/10 step 1200/2391 loss_mean=0.4218
[Train] epoch 1/10 step 1300/2391 loss_mean=0.4380
[Train] epoch 1/10 step 1400/2391 loss_mean=0.4392
[Train] epoch 1/10 step 1500/2391 loss_mean=0.4359
[Train] epoch 1/10 step 1600/2391 loss_mean=0.4281
[Train] epoch 1/10 step 1700/2391 loss_mean=0.4558
[Train] epoch 1/10 step 1800/2391 loss_mean=0.4302
[Train] epoch 1/10 step 1900/2391 loss_mean=0.4317
[Train] epoch 1/10 step 2000/2391 loss_mean=0.4258
[Train] epoch 1/10 step 2100/2391 loss_mean=0.4434
[Train] epoch 1/10 step 2200/2391 loss_mean=0.4395
[Train] epoch 1/10 step 2300/2391 loss_mean=0.4325
[Val]   epoch 1/10 step 100/299 loss_mean=0.2171
[Val]   epoch 1/10 step 200/299 loss_mean=0.2178
Train Loss: 0.4447 | Train Acc: 0.8290
Val Loss: 0.4406 | Val   Acc: 0.8301
 Best model saved to models/xceptionNet/V1/model_df.pt (Val Acc: 0.8301)

EPOCH 2/10
------------------------------
[Train] epoch 2/10 step 100/2391 loss_mean=0.4369
[Train] epoch 2/10 step 200/2391 loss_mean=0.4340
[Train] epoch 2/10 step 300/2391 loss_mean=0.4326
[Train] epoch 2/10 step 400/2391 loss_mean=0.4288
[Train] epoch 2/10 step 500/2391 loss_mean=0.4335
[Train] epoch 2/10 step 600/2391 loss_mean=0.4359
[Train] epoch 2/10 step 700/2391 loss_mean=0.4163
[Train] epoch 2/10 step 800/2391 loss_mean=0.4389
[Train] epoch 2/10 step 900/2391 loss_mean=0.4190
[Train] epoch 2/10 step 1000/2391 loss_mean=0.4003
[Train] epoch 2/10 step 1100/2391 loss_mean=0.4271
[Train] epoch 2/10 step 1200/2391 loss_mean=0.4223
[Train] epoch 2/10 step 1300/2391 loss_mean=0.4434
[Train] epoch 2/10 step 1400/2391 loss_mean=0.4496
[Train] epoch 2/10 step 1500/2391 loss_mean=0.4307
[Train] epoch 2/10 step 1600/2391 loss_mean=0.4259
[Train] epoch 2/10 step 1700/2391 loss_mean=0.4175
[Train] epoch 2/10 step 1800/2391 loss_mean=0.4204
[Train] epoch 2/10 step 1900/2391 loss_mean=0.4294
[Train] epoch 2/10 step 2000/2391 loss_mean=0.4268
[Train] epoch 2/10 step 2100/2391 loss_mean=0.4429
[Train] epoch 2/10 step 2200/2391 loss_mean=0.4255
[Train] epoch 2/10 step 2300/2391 loss_mean=0.4318
[Val]   epoch 2/10 step 100/299 loss_mean=0.2220
[Val]   epoch 2/10 step 200/299 loss_mean=0.2215
Train Loss: 0.4284 | Train Acc: 0.8296
Val Loss: 0.4357 | Val   Acc: 0.8300

EPOCH 3/10
------------------------------
[Train] epoch 3/10 step 100/2391 loss_mean=0.4264
[Train] epoch 3/10 step 200/2391 loss_mean=0.4268
[Train] epoch 3/10 step 300/2391 loss_mean=0.4195
[Train] epoch 3/10 step 400/2391 loss_mean=0.4194
[Train] epoch 3/10 step 500/2391 loss_mean=0.4447
[Train] epoch 3/10 step 600/2391 loss_mean=0.4129
[Train] epoch 3/10 step 700/2391 loss_mean=0.4236
[Train] epoch 3/10 step 800/2391 loss_mean=0.4285
[Train] epoch 3/10 step 900/2391 loss_mean=0.4374
[Train] epoch 3/10 step 1000/2391 loss_mean=0.4122
[Train] epoch 3/10 step 1100/2391 loss_mean=0.4211
[Train] epoch 3/10 step 1200/2391 loss_mean=0.4357
[Train] epoch 3/10 step 1300/2391 loss_mean=0.4043
[Train] epoch 3/10 step 1400/2391 loss_mean=0.4217
[Train] epoch 3/10 step 1500/2391 loss_mean=0.4283
[Train] epoch 3/10 step 1600/2391 loss_mean=0.4293
[Train] epoch 3/10 step 1700/2391 loss_mean=0.4238
[Train] epoch 3/10 step 1800/2391 loss_mean=0.4090
[Train] epoch 3/10 step 1900/2391 loss_mean=0.4418
[Train] epoch 3/10 step 2000/2391 loss_mean=0.4337
[Train] epoch 3/10 step 2100/2391 loss_mean=0.4092
[Train] epoch 3/10 step 2200/2391 loss_mean=0.4061
[Train] epoch 3/10 step 2300/2391 loss_mean=0.4447
[Val]   epoch 3/10 step 100/299 loss_mean=0.2260
[Val]   epoch 3/10 step 200/299 loss_mean=0.2243
Train Loss: 0.4240 | Train Acc: 0.8306
Val Loss: 0.4340 | Val   Acc: 0.8302
 Best model saved to models/xceptionNet/V1/model_df.pt (Val Acc: 0.8302)

EPOCH 4/10
------------------------------
[Train] epoch 4/10 step 100/2391 loss_mean=0.4277
[Train] epoch 4/10 step 200/2391 loss_mean=0.4288
[Train] epoch 4/10 step 300/2391 loss_mean=0.4185
[Train] epoch 4/10 step 400/2391 loss_mean=0.4068
[Train] epoch 4/10 step 500/2391 loss_mean=0.4208
[Train] epoch 4/10 step 600/2391 loss_mean=0.4314
[Train] epoch 4/10 step 700/2391 loss_mean=0.4192
[Train] epoch 4/10 step 800/2391 loss_mean=0.4204
[Train] epoch 4/10 step 900/2391 loss_mean=0.4108
[Train] epoch 4/10 step 1000/2391 loss_mean=0.4136
[Train] epoch 4/10 step 1100/2391 loss_mean=0.4252
[Train] epoch 4/10 step 1200/2391 loss_mean=0.4246
[Train] epoch 4/10 step 1300/2391 loss_mean=0.4332
[Train] epoch 4/10 step 1400/2391 loss_mean=0.4348
[Train] epoch 4/10 step 1500/2391 loss_mean=0.4248
[Train] epoch 4/10 step 1600/2391 loss_mean=0.4142
[Train] epoch 4/10 step 1700/2391 loss_mean=0.4053
[Train] epoch 4/10 step 1800/2391 loss_mean=0.4321
[Train] epoch 4/10 step 1900/2391 loss_mean=0.4232
[Train] epoch 4/10 step 2000/2391 loss_mean=0.4145
[Train] epoch 4/10 step 2100/2391 loss_mean=0.4181
[Train] epoch 4/10 step 2200/2391 loss_mean=0.4295
[Train] epoch 4/10 step 2300/2391 loss_mean=0.4112
[Val]   epoch 4/10 step 100/299 loss_mean=0.2155
[Val]   epoch 4/10 step 200/299 loss_mean=0.2138
Train Loss: 0.4211 | Train Acc: 0.8309
Val Loss: 0.4310 | Val   Acc: 0.8310
 Best model saved to models/xceptionNet/V1/model_df.pt (Val Acc: 0.8310)

EPOCH 5/10
------------------------------
[Train] epoch 5/10 step 100/2391 loss_mean=0.4091
[Train] epoch 5/10 step 200/2391 loss_mean=0.4151
[Train] epoch 5/10 step 300/2391 loss_mean=0.4300
[Train] epoch 5/10 step 400/2391 loss_mean=0.4286
[Train] epoch 5/10 step 500/2391 loss_mean=0.4050
[Train] epoch 5/10 step 600/2391 loss_mean=0.4126
[Train] epoch 5/10 step 700/2391 loss_mean=0.4186
[Train] epoch 5/10 step 800/2391 loss_mean=0.4061
[Train] epoch 5/10 step 900/2391 loss_mean=0.4229
[Train] epoch 5/10 step 1000/2391 loss_mean=0.4135
[Train] epoch 5/10 step 1100/2391 loss_mean=0.4076
[Train] epoch 5/10 step 1200/2391 loss_mean=0.4169
[Train] epoch 5/10 step 1300/2391 loss_mean=0.4311
[Train] epoch 5/10 step 1400/2391 loss_mean=0.4274
[Train] epoch 5/10 step 1500/2391 loss_mean=0.4306
[Train] epoch 5/10 step 1600/2391 loss_mean=0.4180
[Train] epoch 5/10 step 1700/2391 loss_mean=0.4313
[Train] epoch 5/10 step 1800/2391 loss_mean=0.4163
[Train] epoch 5/10 step 1900/2391 loss_mean=0.4167
[Train] epoch 5/10 step 2000/2391 loss_mean=0.4063
[Train] epoch 5/10 step 2100/2391 loss_mean=0.4204
[Train] epoch 5/10 step 2200/2391 loss_mean=0.4124
[Train] epoch 5/10 step 2300/2391 loss_mean=0.4245
[Val]   epoch 5/10 step 100/299 loss_mean=0.2292
[Val]   epoch 5/10 step 200/299 loss_mean=0.2266
Train Loss: 0.4184 | Train Acc: 0.8313
Val Loss: 0.4317 | Val   Acc: 0.8321
 Best model saved to models/xceptionNet/V1/model_df.pt (Val Acc: 0.8321)

EPOCH 6/10
------------------------------
[Train] epoch 6/10 step 100/2391 loss_mean=0.4173
[Train] epoch 6/10 step 200/2391 loss_mean=0.4078
[Train] epoch 6/10 step 300/2391 loss_mean=0.4127
[Train] epoch 6/10 step 400/2391 loss_mean=0.4398
[Train] epoch 6/10 step 500/2391 loss_mean=0.4357
[Train] epoch 6/10 step 600/2391 loss_mean=0.4181
[Train] epoch 6/10 step 700/2391 loss_mean=0.4121
[Train] epoch 6/10 step 800/2391 loss_mean=0.4107
[Train] epoch 6/10 step 900/2391 loss_mean=0.4108
[Train] epoch 6/10 step 1000/2391 loss_mean=0.4185
[Train] epoch 6/10 step 1100/2391 loss_mean=0.4288
[Train] epoch 6/10 step 1200/2391 loss_mean=0.4223
[Train] epoch 6/10 step 1300/2391 loss_mean=0.4247
[Train] epoch 6/10 step 1400/2391 loss_mean=0.4152
[Train] epoch 6/10 step 1500/2391 loss_mean=0.4216
[Train] epoch 6/10 step 1600/2391 loss_mean=0.4244
[Train] epoch 6/10 step 1700/2391 loss_mean=0.4175
[Train] epoch 6/10 step 1800/2391 loss_mean=0.4164
[Train] epoch 6/10 step 1900/2391 loss_mean=0.4030
[Train] epoch 6/10 step 2000/2391 loss_mean=0.4037
[Train] epoch 6/10 step 2100/2391 loss_mean=0.4277
[Train] epoch 6/10 step 2200/2391 loss_mean=0.4083
[Train] epoch 6/10 step 2300/2391 loss_mean=0.4182
[Val]   epoch 6/10 step 100/299 loss_mean=0.2211
[Val]   epoch 6/10 step 200/299 loss_mean=0.2184
Train Loss: 0.4173 | Train Acc: 0.8311
Val Loss: 0.4289 | Val   Acc: 0.8320

EPOCH 7/10
------------------------------
[Train] epoch 7/10 step 100/2391 loss_mean=0.4176
[Train] epoch 7/10 step 200/2391 loss_mean=0.4179
[Train] epoch 7/10 step 300/2391 loss_mean=0.4077
[Train] epoch 7/10 step 400/2391 loss_mean=0.4163
[Train] epoch 7/10 step 500/2391 loss_mean=0.4107
[Train] epoch 7/10 step 600/2391 loss_mean=0.4253
[Train] epoch 7/10 step 700/2391 loss_mean=0.4054
[Train] epoch 7/10 step 800/2391 loss_mean=0.4227
[Train] epoch 7/10 step 900/2391 loss_mean=0.4224
[Train] epoch 7/10 step 1000/2391 loss_mean=0.4293
[Train] epoch 7/10 step 1100/2391 loss_mean=0.4325
[Train] epoch 7/10 step 1200/2391 loss_mean=0.4277
[Train] epoch 7/10 step 1300/2391 loss_mean=0.4219
[Train] epoch 7/10 step 1400/2391 loss_mean=0.3952
[Train] epoch 7/10 step 1500/2391 loss_mean=0.4071
[Train] epoch 7/10 step 1600/2391 loss_mean=0.4259
[Train] epoch 7/10 step 1700/2391 loss_mean=0.4092
[Train] epoch 7/10 step 1800/2391 loss_mean=0.4254
[Train] epoch 7/10 step 1900/2391 loss_mean=0.4118
[Train] epoch 7/10 step 2000/2391 loss_mean=0.4177
[Train] epoch 7/10 step 2100/2391 loss_mean=0.4144
[Train] epoch 7/10 step 2200/2391 loss_mean=0.3990
[Train] epoch 7/10 step 2300/2391 loss_mean=0.4219
[Val]   epoch 7/10 step 100/299 loss_mean=0.2414
[Val]   epoch 7/10 step 200/299 loss_mean=0.2388
Train Loss: 0.4167 | Train Acc: 0.8316
Val Loss: 0.4326 | Val   Acc: 0.8319

EPOCH 8/10
------------------------------
[Train] epoch 8/10 step 100/2391 loss_mean=0.4228
[Train] epoch 8/10 step 200/2391 loss_mean=0.4307
[Train] epoch 8/10 step 300/2391 loss_mean=0.4267
[Train] epoch 8/10 step 400/2391 loss_mean=0.3975
[Train] epoch 8/10 step 500/2391 loss_mean=0.4063
[Train] epoch 8/10 step 600/2391 loss_mean=0.4136
[Train] epoch 8/10 step 700/2391 loss_mean=0.4070
[Train] epoch 8/10 step 800/2391 loss_mean=0.4237
[Train] epoch 8/10 step 900/2391 loss_mean=0.4262
[Train] epoch 8/10 step 1000/2391 loss_mean=0.4093
[Train] epoch 8/10 step 1100/2391 loss_mean=0.4178
[Train] epoch 8/10 step 1200/2391 loss_mean=0.4240
[Train] epoch 8/10 step 1300/2391 loss_mean=0.4273
[Train] epoch 8/10 step 1400/2391 loss_mean=0.4054
[Train] epoch 8/10 step 1500/2391 loss_mean=0.4078
[Train] epoch 8/10 step 1600/2391 loss_mean=0.4126
[Train] epoch 8/10 step 1700/2391 loss_mean=0.3865
[Train] epoch 8/10 step 1800/2391 loss_mean=0.4211
[Train] epoch 8/10 step 1900/2391 loss_mean=0.4396
[Train] epoch 8/10 step 2000/2391 loss_mean=0.4248
[Train] epoch 8/10 step 2100/2391 loss_mean=0.4200
[Train] epoch 8/10 step 2200/2391 loss_mean=0.4112
[Train] epoch 8/10 step 2300/2391 loss_mean=0.4159
[Val]   epoch 8/10 step 100/299 loss_mean=0.2006
[Val]   epoch 8/10 step 200/299 loss_mean=0.1989
Train Loss: 0.4161 | Train Acc: 0.8316
Val Loss: 0.4261 | Val   Acc: 0.8314

EPOCH 9/10
------------------------------
[Train] epoch 9/10 step 100/2391 loss_mean=0.4103
[Train] epoch 9/10 step 200/2391 loss_mean=0.4233
[Train] epoch 9/10 step 300/2391 loss_mean=0.4045
[Train] epoch 9/10 step 400/2391 loss_mean=0.4076
[Train] epoch 9/10 step 500/2391 loss_mean=0.4185
[Train] epoch 9/10 step 600/2391 loss_mean=0.4110
[Train] epoch 9/10 step 700/2391 loss_mean=0.4133
[Train] epoch 9/10 step 800/2391 loss_mean=0.4134
[Train] epoch 9/10 step 900/2391 loss_mean=0.4205
[Train] epoch 9/10 step 1000/2391 loss_mean=0.4127
[Train] epoch 9/10 step 1100/2391 loss_mean=0.4297
[Train] epoch 9/10 step 1200/2391 loss_mean=0.4262
[Train] epoch 9/10 step 1300/2391 loss_mean=0.4213
[Train] epoch 9/10 step 1400/2391 loss_mean=0.4171
[Train] epoch 9/10 step 1500/2391 loss_mean=0.4210
[Train] epoch 9/10 step 1600/2391 loss_mean=0.3982
[Train] epoch 9/10 step 1700/2391 loss_mean=0.4141
[Train] epoch 9/10 step 1800/2391 loss_mean=0.4343
[Train] epoch 9/10 step 1900/2391 loss_mean=0.4065
[Train] epoch 9/10 step 2000/2391 loss_mean=0.4205
[Train] epoch 9/10 step 2100/2391 loss_mean=0.4187
[Train] epoch 9/10 step 2200/2391 loss_mean=0.4205
[Train] epoch 9/10 step 2300/2391 loss_mean=0.4131
[Val]   epoch 9/10 step 100/299 loss_mean=0.2045
[Val]   epoch 9/10 step 200/299 loss_mean=0.2027
Train Loss: 0.4161 | Train Acc: 0.8316
Val Loss: 0.4261 | Val   Acc: 0.8323
 Best model saved to models/xceptionNet/V1/model_df.pt (Val Acc: 0.8323)

EPOCH 10/10
------------------------------
[Train] epoch 10/10 step 100/2391 loss_mean=0.3988
[Train] epoch 10/10 step 200/2391 loss_mean=0.4240
[Train] epoch 10/10 step 300/2391 loss_mean=0.4147
[Train] epoch 10/10 step 400/2391 loss_mean=0.4282
[Train] epoch 10/10 step 500/2391 loss_mean=0.3961
[Train] epoch 10/10 step 600/2391 loss_mean=0.4029
[Train] epoch 10/10 step 700/2391 loss_mean=0.4170
[Train] epoch 10/10 step 800/2391 loss_mean=0.4020
[Train] epoch 10/10 step 900/2391 loss_mean=0.4242
[Train] epoch 10/10 step 1000/2391 loss_mean=0.4147
[Train] epoch 10/10 step 1100/2391 loss_mean=0.3992
[Train] epoch 10/10 step 1200/2391 loss_mean=0.4142
[Train] epoch 10/10 step 1300/2391 loss_mean=0.4264
[Train] epoch 10/10 step 1400/2391 loss_mean=0.4066
[Train] epoch 10/10 step 1500/2391 loss_mean=0.4138
[Train] epoch 10/10 step 1600/2391 loss_mean=0.4392
[Train] epoch 10/10 step 1700/2391 loss_mean=0.4272
[Train] epoch 10/10 step 1800/2391 loss_mean=0.4157
[Train] epoch 10/10 step 1900/2391 loss_mean=0.4469
[Train] epoch 10/10 step 2000/2391 loss_mean=0.4247
[Train] epoch 10/10 step 2100/2391 loss_mean=0.4049
[Train] epoch 10/10 step 2200/2391 loss_mean=0.4017
[Train] epoch 10/10 step 2300/2391 loss_mean=0.4270
[Val]   epoch 10/10 step 100/299 loss_mean=0.2414
[Val]   epoch 10/10 step 200/299 loss_mean=0.2390
Train Loss: 0.4161 | Train Acc: 0.8319
Val Loss: 0.4319 | Val   Acc: 0.8319
Melhor acc: 0.8323

Tempo total de treino: 156m 44s
